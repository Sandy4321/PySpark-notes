{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalando y ligando Apache-Spark (pyspark) a Pycharm y Jupyter\n",
    "\n",
    "Primero vamos a descargar spark:\n",
    "\n",
    "Spark puede ser ejecutado en un cluster en **modo local**, esto significa que todos los procesos de spark corren en la misma JVM. Una de las ventajas de usar este **modo local** es que se puede debuggear el trabajo y probarlo, este modo tambien puede ser util en escenarios donde queremos ejecutar computo en paralelo a traves de diferentes procesadores en una sola computadora:\n",
    "\n",
    "### Para Pycharm:\n",
    "\n",
    "### Paso 1\n",
    "\n",
    "En mac, abrimos la terminal e instalamos con homebrew:\n",
    "\n",
    "1. `brew install apache-spark`\n",
    "2. `brew install hadoop`\n",
    "\n",
    "### Paso 2\n",
    "Luego obtenemos el path para configurar en pycharm, en la terminal:\n",
    "\n",
    "1. brew info apache-spark\n",
    "\n",
    "```user@MacBook-Pro-de-User-2:~ brew info apache-spark\n",
    "apache-spark: stable 1.6.0, HEAD Engine for large-scale data processing\n",
    "https://spark.apache.org/\n",
    "/usr/local/Cellar/apache-spark/1.5.1 (649 files, 302.9M) *\n",
    "  Poured from bottle\n",
    "From: https://github.com/Homebrew/homebrew/blob/master/Library/Formula/apache-spark.rb\n",
    "```\n",
    "\n",
    "### Paso 3\n",
    "1. Luego vamos a Pycharm de lado superior: Run>Edit Configurations >Environment Variables. \n",
    "2. Creamos dos nuevas variables PYTHONPATH Y SPARKHOME\n",
    "3. En PYTHONPATH ( /usr/local/Cellar/apache-spark/1.5.1/libexec/python )\n",
    "4. EN SPARKHOME ( /usr/local/Cellar/apache-spark/1.5.1 )\n",
    "\n",
    "Finalmente parece que todo esta listo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Por otro lado, tambien hubieramos podido usar el siguiente script:\n",
    "\n",
    "```\n",
    "import os\n",
    "import sys\n",
    "\n",
    " # Path for spark source folder\n",
    " #/usr/local/Cellar/apache-spark/1.5.1\n",
    "os.environ['SPARK_HOME']=\"/usr/local/Cellar/apache-spark/1.5.1\"\n",
    "\n",
    " # Append pyspark  to Python Path\n",
    "sys.path.append(\"/usr/local/Cellar/apache-spark/1.5.1/libexec/python\")\n",
    "\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark import SparkConf\n",
    "    print (\"Successfully imported Spark Modules\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print (\"Can not import Spark Modules\", e)\n",
    "    sys.exit(1)\n",
    "```\n",
    "\n",
    "### Para Jupyter:\n",
    "\n",
    "Ahora para ligar pyspark con jupyter hacemos lo siguiente en la terminal (asegurarse de revisar las versiones de apache-spark):\n",
    "\n",
    "### Paso 1\n",
    "\n",
    "```user@MacBook-Pro-de-User-2:~$ echo \"export PATH=$PATH:/usr/local/Cellar/apache-spark/1.5.1/bin\" >> .profile\n",
    "user@MacBook-Pro-de-User-2:~$ echo \"export PYSPARK_DRIVER_PYTHON=ipython\" >> .profile\n",
    "user@MacBook-Pro-de-User-2:~$ echo \"export PYSPARK_DRIVER_PYTHON_OPTS='notebook' pyspark\" >> .profile\n",
    "user@MacBook-Pro-de-User-2:~$ source .profile```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Paso 2\n",
    "Ahora abrimos en la terminal un notebook y iniciado pyspark:\n",
    "\n",
    "```\n",
    "user@MacBook-Pro-de-User-2:~$ cd Jupyter/\n",
    "user@MacBook-Pro-de-User-2:~/Jupyter$ pyspark\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos probar con la variable spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x1073b3e90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambos quedaron ligados e instalados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando y visualizando datos con apache-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo usaremos The MovieLens 100k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1|24|M|technician|85711'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data = sc.textFile('/Users/user/Downloads/ml-100k/u.user')\n",
    "user_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>24</th>\n",
       "      <th>M</th>\n",
       "      <th>technician</th>\n",
       "      <th>85711</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>94043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>writer</td>\n",
       "      <td>32067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>technician</td>\n",
       "      <td>43537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>other</td>\n",
       "      <td>15213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>M</td>\n",
       "      <td>executive</td>\n",
       "      <td>98101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  24  M  technician  85711\n",
       "0  2  53  F       other  94043\n",
       "1  3  23  M      writer  32067\n",
       "2  4  24  M  technician  43537\n",
       "3  5  33  F       other  15213\n",
       "4  6  42  M   executive  98101"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aunque tambien lo hubieramos podido hacer con pandas\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/user/Downloads/ml-100k/u.user', sep='|')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[12] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_fields = user_data.map(lambda line :\n",
    "                           line.split('|'))\n",
    "user_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 943, genders:2, occupations:21, Codigos postales:795\n"
     ]
    }
   ],
   "source": [
    "num_users = user_fields.map(lambda fields:\n",
    "                           fields[0]).distinct().count()\n",
    "\n",
    "num_genders = user_fields.map(lambda fields:\n",
    "                             fields[2]).distinct().count()\n",
    "\n",
    "num_ocupations = user_fields.map(lambda fields:\n",
    "                                fields[3]).distinct().count()\n",
    "\n",
    "num_zipcodes = user_fields.map(lambda fields:\n",
    "                              fields[4]).distinct().count()\n",
    "\n",
    "print 'Users: %d, genders:%d, occupations:%d, Codigos postales:%d'%(\n",
    "    num_users, num_genders, num_ocupations, num_zipcodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Figure.show of <matplotlib.figure.Figure object at 0x109051b10>>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109051b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "from IPython.display import Image\n",
    "import matplotlib as mlp\n",
    "ages = user_fields.map(lambda x: int(x[1])).collect\n",
    "fig = mlp.pyplot.gcf()\n",
    "fig.set_size_inches(16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
